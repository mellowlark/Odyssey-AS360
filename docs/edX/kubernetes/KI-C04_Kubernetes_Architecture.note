KI Chapter 4: Kubernetes Architecture

In this chapter, we will explore the Kubernetes architecture.
The components of a control plane node, the role of the worker nodes.
The cluster state management with etcd and the network setup requirements. 
We will also learn about the Container Network Interface (CNI), as Kubernetes' network specification

Objectives
  Discuss the Kubernetes architecture.
  Explain the different components of the control plane and worker nodes.
  Discuss cluster state management with etcd.
  Review the Kubernetes network setup requirements.

At a very high level, Kubernetes is a cluster of compute systems categorized by their distinct roles:
One or more control plane nodes
One or more worker nodes (optional, but recommended).

Control Plane Node Overview
Most important node. Responsible for managing all other nodes.
All and only cluster configuration data is stored in the key-value store (etcd I believe)
This store can be managed inside the control plane or externally for a bit more safety.

A control plane node runs the following essential control plane components and agents:
  API Server
  Scheduler
  Controller Managers
  Key-Value Data Store

In addition, the control plane node runs:
  Container Runtime
  Node Agent - kubelet
  Proxy - kube-proxy
  Optional add-ons for observability, such as dashboards, cluster-level monitoring, and logging

Components in depth
===================

Control Plane Nodes
====================

-- kube-apiserver --
Pretty much does what an API server does.
Can also act as a proxy to other API custom servers

-- Scheduler --
The API server passes iformaiton to the kube-scheduler which in turn passes out jobs to the pods back through the API server.
The scheduler takes into account Quality of Service (QoS) requirements, data locality, affinity, 
anti-affinity, taints, toleration, cluster topology, etc. 
Once all the cluster data is available, the scheduling algorithm filters the nodes with predicates 
to isolate the possible node candidates which then are scored with priorities in order to select 
the node that satisfies all the requirements for hosting the new workload. 

-- Controller Managers --
Make sure things are runing smooth and if they arn't they take action to get them back to doing so.
Kube control manager watch the cluster. cloud conroller managers watch the cloud provider details.
Load balancing, system volumes, routiung etc....

-- Key-Value Data Store --  
etcd
from etcd.io 
A strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by 
a distributed system or cluster of machines. 
It gracefully handles leader elections during network partitions and can tolerate machine failure, even in the leader node. 

etcd's CLI management tool is called etcdctl

Only the API Server is able to communicate with the etcd data store.

etcd is based on the Raft Consensus Algorithm which allows a collection of machins to
work as a coherent group that can survive the failures of some of its members.

See https://raft.github.io/ for more info on the raft Consensus



Worker Nodes
=============

-- Worker Node Overview --

 A Pod is the smallest scheduling work unit in Kubernetes. 
 It is a logical collection of one or more containers scheduled together, 
 and the collection can be started, stopped, or rescheduled as a single unit of work. 

In a multi-worker Kubernetes cluster, the network traffic between client users and the
containerized applications deployed in Pods is handled directly by the worker nodes.
It is not routed through the control plane node.


-- Worker Node Components  --
    Container Runtime
    Node Agent - kubelet
    Proxy - kube-proxy
    Add-ons for DNS, observability components such as dashboards, cluster-level monitoring and logging, and device plugins.

-- Container Runtime --
Kubernetes lacks the capability to directly handle and run containers. Which is why it reqires a separate runtime.
Kubernetes supports several container runtimes.
  CRI-IO A lightweight container runtime that supports quay.io and Docker Hub image registries
  containered 
  Docker Engine
  Mirantis Container Runtime

-- Node Agent - kubelet --
Kubelet is an agent and runs on each note. Both worker and control nodes.
Connects to container runtimes through CRI (container runtime interface)
The CRI consists of protocol buffers, gRPC API, libraries and other tools.

    Kubelet (gRPC client) <--> CRI shim (gRPC server) <--> container runtime -->>>> container

This connection is run through a CRI shim (which seems to include gRPC)
The kubelet acting as grpc client connects to the CRI shim acting as grpc server to perform container and image operations. 
The CRI implements two services: ImageService and RuntimeService. 
The ImageService is responsible for all the image-related operations,
while the RuntimeService is responsible for all the Pod and container-related operations.

--  kubelet - CRI shims --

Originally the kubelet agent supported only a couple of container runtimes.
First the Docker Engine followed by rkt, through a unique interface model integrated directly in the kubelet source code.
In time, Kubernetes started migrating towards a standardized approach, CRI. 
Any container runtime that implements the CRI could be used by Kubernetes to manage containers.

Shims are Container Runtime Interface (CRI) implementations, interfaces or adapters, specific to each container runtime supported by Kubernetes.

cri-containerd
cri-containerd allows containers to be directly created and managed with containerd at kubelet's request.
cri-conaintered: Kubelet <--> cri-containered <--> containered -->>>> container

-- CRI-O --

Enables the use of any Open Container Initiative (OCI) compatable runtime, such as runC.

dockershim replaced with cri-dockered
Docker kubelet <-CRI-> dockershim <--> docker <--containerD-->>>>containers
Docker kubelet <-CRI-> cri-dockered <--> docker <--containerD-->>>>containers

-- Proxy - kube-proxy --

The network ageint that runs on each node. Both the control and worker nodes.
Responsible for updating and maintaining networking rules.
Works in conjuntion with Linux's Iptables firewall utility.

-- Add-ons --

Cluster features and functionality, not yet available in Kubernetes.
  DNS:  Cluster DNS is a DNS server required to assign DNS records to Kubernetes objects.
  Dashboard: A general purpose web-based user interface
  Monitoring: Collects cluster-level xcontainer metrics and saves them to a data store.
  Logging: Collects cluster-level container logs and saves them to a central log store.
  Device plugins: For system hardware resources.


Networking Challeges
====================

Kuberneties requires a lot of networking to mimic monolithic application communication
Container to container, pod to pod, service to pod nodes and external communication.

-- Container-to-Container Communication Inside Pods --
Network namespace can be shared across containers or with host OS.
A special ifrastructure called Pause container for creating a network namespace for the Pod.

-- Pod-to-Pod Communication Across Nodes --
Pods are expected to be able to communicate with all other Pods in the cluster, all this without the implementation of Network Address Translation (NAT)
The Kubernetes network model aims to reduce complexity, and it treats Pods as VMs on a network.
Each VM is equiped with a network interface so each recievs a unique IP.
The model is called IP-per-Pod.

Containers also share the Pods network namespace and need to coordinate ports inide the pods.
They also need to be able to communicate with themselves via localost.
Containers are intergrated with overall networking through the use of Container Network Interface (CNI) supprted by CNI plugins.
CNI sounds like it may be a set of rules for other's to use when building a CNI plugin.
Most of these plugins are 3rd party. Very few are maintained by the CNI team. These are known as Software Defned Networking (SDN)
Some solutions offer support for Network Policies. Flannel, Weave, Calico, and Cilium are a few of the SDN solutions available for Kubernetes clusters.

-- External-to-Pod Communication --
External accessibility is enabled through Services, complex encapsulations of network routing rule definitions stored in iptables on cluster nodes and implemented by kube-proxy agents.


